from fastapi import APIRouter, Depends
from pydantic import BaseModel
from fastapi.responses import JSONResponse
import re
from app.core.security import get_current_user
from app.models.user import User
import asyncio
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.tools import tool
from sqlalchemy.orm import Session
from app.models.kabis import Kabis
from app.models.libtau import Library
import time
from fastapi import HTTPException
from pydantic import BaseModel
from typing import List, Optional
from ...deps import get_retriever_dep, get_llm, get_book_retriever_dep
from app.models.chat import ChatHistory
from app.core.db import SessionLocal
import re
from fastapi.responses import StreamingResponse
from app.models.books import Document
from sentence_transformers import CrossEncoder


router = APIRouter(prefix="/api", tags=["chat", "chat_card"])
reranker = CrossEncoder("BAAI/bge-reranker-v2-m3")


def clean_context(text: str) -> str:
    """
    –£–¥–∞–ª—è–µ—Ç —Ä–∞–∑–¥–µ–ª—ã —Å–æ —Å–ø–∏—Å–∫–∞–º–∏ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –∏ –≤–Ω–µ—à–Ω–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
    (–≤–∫–ª—é—á–∞—è '–°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã', '–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', 'References' –∏ –∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç—ã)
    –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫–Ω–∏–≥–∏.
    """
    # –°–Ω–∞—á–∞–ª–∞ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã (—á—Ç–æ–±—ã '–ª–∏—Ç–µ—Ä –∞—Ç—É—Ä–∞' —Å—Ç–∞–ª–æ '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞')
    normalized = re.sub(r'\s+', ' ', text)

    # –£–¥–∞–ª—è–µ–º –≤—Å—ë, —á—Ç–æ –∏–¥—ë—Ç –ø–æ—Å–ª–µ —Ç–∏–ø–∏—á–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å–ø–∏—Å–∫–æ–≤ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã
    pattern = r'(?i)(—Å–ø–∏—Å–æ–∫\s+—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–æ–π\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã|—Å–ø–∏—Å–æ–∫\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã|–æ—Å–Ω–æ–≤–Ω–∞—è\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞|–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞|–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞|references)\b.*'
    cleaned = re.sub(pattern, '', normalized)

    return cleaned.strip()


def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


class ChatHistoryItem(BaseModel):
    sessionId: str
    question: str
    answer: str
    tools_used: Optional[List[str]] = []
    timestamp: Optional[float] = None  # UNIX timestamp


def save_chat_history(db: Session, session_id: str, question: str, answer: str, tools_used: list[str]):
    item = ChatHistory(
        session_id=session_id,
        question=question,
        answer=answer,
        tools_used=tools_used,
        timestamp=time.time()
    )
    db.add(item)
    db.commit()
    db.refresh(item)
    return item


class ChatRequest(BaseModel):
    query: str
    k: int | None = None
    sessionId: Optional[str] = None  # –Ω–æ–≤–æ–µ –ø–æ–ª–µ


def _format_docs(docs, per_chunk_chars=800, max_chunks=5):
    lines = []
    for d in docs[:max_chunks]:
        m = d.metadata or {}
        title = m.get("title", "–∫–Ω–∏–≥–∞")
        # author = m.get("author", "–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω")
        # subject = m.get("subject")
        page = m.get("page", "?")
        text = (d.page_content or "")[:per_chunk_chars].strip()
        lines.append(f"[{title}, —Å—Ç—Ä. {page}] {text}")
        print(title, m.get("source"), text)
    return "\n\n".join(lines)


# ---- –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç: –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ ----
@tool("vector_search", return_direct=False)
def vector_search(query: str, k: int = 5, retriever=None) -> str:
    """
    –ü–æ–∏—Å–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ –ø–æ —Å–º—ã—Å–ª–æ–≤–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫—É—Å–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∫–Ω–∏–≥–∏, –∞–≤—Ç–æ—Ä–∞ –∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    """
    print("–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫")
    if retriever is None:
        return "Retriever –Ω–µ –ø–æ–¥–∫–ª—é—á—ë–Ω"
    docs = retriever.invoke(query, config={"k": k})
    return _format_docs(docs)


def _format_books(docs, max_items=None):
    lines = []
    limit = max_items or len(docs)   # –µ—Å–ª–∏ None ‚Üí –±–µ—Ä–µ–º –≤—Å–µ
    for d in docs[:limit]:
        m = d.metadata or {}
        title = m.get("title", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–Ω–∏–≥–∞")
        # subject = m.get("subject", "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
        print(title, m.get("title"), m.get("id_book"))

        lines.append(f"üìò {title}\n")
    return "\n\n".join(lines)


@tool("book_search")
def book_search(query: str, k: int = 50, retriever=None) -> str:
    """
    –û–±–∑–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–Ω–∏–≥–∞–º (—ç–º–±–µ–¥–¥–∏–Ω–≥–∏).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–Ω–∏–≥ (–º–Ω–æ–≥–æ).
    """
    print("–û–±–∑–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫")
    if retriever is None:
        return "Retriever –¥–ª—è –∫–Ω–∏–≥ –Ω–µ –ø–æ–¥–∫–ª—é—á—ë–Ω"

    docs = retriever.invoke(query, config={"k": k})
    return _format_books(docs, max_items=k)


# –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
# –∫–ª—é—á: sessionId, –∑–Ω–∞—á–µ–Ω–∏–µ: timestamp –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
_last_request_time: dict[str, float] = {}

RATE_LIMIT_SECONDS = 5  # –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏


@router.post("/chat", summary="–ß–∞—Ç —Å –ò–ò")
async def chat(req: ChatRequest,
               retriever=Depends(get_retriever_dep),
               book_retriever=Depends(get_book_retriever_dep),
               llm=Depends(get_llm),
               db: Session = Depends(get_db)):

    session_id = req.sessionId or "anonymous"

    vs_tool_used = []
    bs_tool_used = []

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–∞
    now = time.time()
    last_time = _last_request_time.get(session_id, 0)

    if now - last_time < RATE_LIMIT_SECONDS:
        return JSONResponse(
            {"error": f"–°–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ {int(RATE_LIMIT_SECONDS - (now - last_time))} —Å–µ–∫."},
            status_code=429
        )

    _last_request_time[session_id] = now

    # tools
    vs_tool = lambda q, k=5: (vs_tool_used.append("vector_search") or vector_search.func(q, k, retriever=retriever))
    bs_tool = lambda q, k=100: (bs_tool_used.append("book_search") or book_search.func(q, k, retriever=book_retriever))

    # –û–±—â–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    system_prompt = (
        "–¢—ã ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –ø–æ–∏—Å–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–Ω–∏–≥–∞—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ ¬´–¢—É—Ä–∞–Ω-–ê—Å—Ç–∞–Ω–∞¬ª.\n"
        "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ ¬´–ö–æ–Ω—Ç–µ–∫—Å—Ç¬ª, –≥–¥–µ —É–∫–∞–∑–∞–Ω—ã –∫–Ω–∏–≥–∏ –∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.\n"
        "–ö–∞–∂–¥—ã–π —Ñ–∞–∫—Ç –∏–ª–∏ –≤—ã–≤–æ–¥ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–π —Å—Å—ã–ª–∫–æ–π –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n"
        "¬´(<i>–Ω–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏</i>, —Å—Ç—Ä. N)¬ª.\n"
        "–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç ‚Äî —á–µ—Å—Ç–Ω–æ —Å–æ–æ–±—â–∞–π: "
        "¬´–í –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ –¢—É—Ä–∞–Ω-–ê—Å—Ç–∞–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç.¬ª\n"
        "–§–æ—Ä–º–∞—Ç–∏—Ä—É–π –æ—Ç–≤–µ—Ç –≤ HTML —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ–≥–æ–≤ (<p>, <ul>, <li>, <b>, <i>).\n"
        "–ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –∫–Ω–∏–≥–∏ –∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ —É–∫–∞–∑–∞–Ω—ã –≤ —Ä–∞–∑–¥–µ–ª–µ ¬´–ö–æ–Ω—Ç–µ–∫—Å—Ç¬ª."
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        (
            "human",
            "–í–æ–ø—Ä–æ—Å —Å—Ç—É–¥–µ–Ω—Ç–∞: {question}\n\n"
            "–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∫–Ω–∏–≥–∏ –∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã):\n{context}"
        ),
    ])

    # --- 1Ô∏è‚É£ –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å ‚Äî —á–µ—Ä–µ–∑ vector_search ---
    vector_chain = (
        RunnableParallel(
            question=RunnablePassthrough(),
            context=lambda q: clean_context(vs_tool(q, req.k or 5)),
        )
        | prompt
        | llm
    )
    vector_answer = vector_chain.invoke(req.query).content

    # --- 2Ô∏è‚É£ –í—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å ‚Äî —á–µ—Ä–µ–∑ book_search ---
    book_chain = (
        RunnableParallel(
            question=RunnablePassthrough(),
            context=lambda q: bs_tool(q, req.k or 100),
        )
        | prompt
        | llm
    )
    book_answer = book_chain.invoke(req.query).content

    # --- 3Ô∏è‚É£ –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Ç–≤–µ—Ç—ã ---
    final_answer = (
        "<h3>–û—Ç–≤–µ—Ç –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫):</h3>\n"
        f"{vector_answer}\n"
        "<hr>"
        "<h3>–û—Ç–≤–µ—Ç –ø–æ –∫–Ω–∏–≥–∞–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:</h3>\n"
        f"{book_answer}"
    )

    # --- 4Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î ---
    save_chat_history(
        db=db,
        session_id=session_id,
        question=req.query,
        answer=final_answer,
        tools_used=list(set(vs_tool_used + bs_tool_used)),
    )

    return {"reply": final_answer}


async def summarize_card(llm, req, card):
    key, value = card
    context = ''
    cards = {

    }
    for i in range(len(value["pages"])):
        context += "—Å—Ç—Ä." + str(value["pages"][i]) + "\n"
        context += "—Ñ—Ä–∞–≥–º–µ–Ω—Ç" + value["text_snippets"][i] + "\n"
    return {
        'title': value["title"],
        'download_url': value['download_url'],
        "text_snippet": context,
            }

from fastapi import Depends
import asyncio
import time
from sqlalchemy.orm import Session

# ... –≤–∞—à–∏ –∏–º–ø–æ—Ä—Ç—ã

@router.post("/chat_card", summary="–ß–∞—Ç —Å –∫–∞—Ä—Ç–æ—á–∫–∞–º–∏ –∫–Ω–∏–≥")
async def chat(
    req: ChatRequest,
    retriever=Depends(get_retriever_dep),
    book_retriever=Depends(get_book_retriever_dep),
    llm=Depends(get_llm),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    session_id = req.sessionId or "anonymous"

    # --- –∑–∞—â–∏—Ç–∞ –æ—Ç —Å–ø–∞–º–∞ ---
    now = time.time()
    last_time = _last_request_time.get(session_id, 0)
    if now - last_time < RATE_LIMIT_SECONDS:
        return JSONResponse(
            {"error": f"–°–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ {int(RATE_LIMIT_SECONDS - (now - last_time))} —Å–µ–∫."},
            status_code=429
        )
    _last_request_time[session_id] = now

    k_vec = req.k or 15

    # --- BOOK SEARCH (async) ---
    try:
        book_docs = await book_retriever.ainvoke(req.query, config={"k": 10})
    except AttributeError:
        # –µ—Å–ª–∏ .ainvoke –Ω–µ—Ç ‚Äî —É–Ω–µ—Å—ë–º sync –≤ threadpool
        book_docs = await asyncio.to_thread(book_retriever.invoke, req.query, {"k": 10})

    id_books_from_book_search = [d.metadata.get("id_book") for d in book_docs if getattr(d, "metadata", None)]

    # --- –ó–∞–ø—Ä–æ—Å Kabis (sync DB -> threadpool) ---
    def fetch_kabis_records(ids):
        with SessionLocal() as session:
            return session.query(Kabis).filter(Kabis.id_book.in_(ids)).all()

    kabis_records = await asyncio.to_thread(fetch_kabis_records, id_books_from_book_search)

    kb_map = [
        {
            "Language": k.lang,
            "title": f"{k.author} {k.title}".strip(),
            "pub_info": k.pub_info,
            "year": k.year,
            "subjects": k.subjects,
            "source": "book_search",
            "id_book": k.id_book,
            "download_url": (k.download_url or None),
        }
        for k in kabis_records
    ]

    # --- –í–ï–ö–¢–û–†–ù–´–ô –ü–û–ò–°–ö (async) ---
    try:
        vec_docs = await retriever.ainvoke(req.query, config={"k": k_vec})
    except AttributeError:
        vec_docs = await asyncio.to_thread(retriever.invoke, req.query, {"k": k_vec})

    # --- –†–ï–†–ê–ù–ö–ï–† (sync -> threadpool) ---
    pairs = [(req.query, (d.page_content or "")) for d in vec_docs]
    # –í—ã–Ω–µ—Å–µ–º predict –≤ threadpool, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
    scores = await asyncio.to_thread(reranker.predict, pairs)

    for d, s in zip(vec_docs, scores):
        # –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ metadata –µ—Å—Ç—å
        if not getattr(d, "metadata", None):
            d.metadata = {}
        d.metadata["rerank_score"] = float(s)

    vec_docs.sort(key=lambda x: x.metadata.get("rerank_score", 0.0), reverse=True)

    # --- –°–æ–±–∏—Ä–∞–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –ø–æ id_book (–µ–¥–∏–Ω—ã–π –∫–ª—é—á!) ---
    vector_cards_by_book: dict[str, dict] = {}
    for d in vec_docs:
        m = d.metadata or {}
        id_book = m.get("id_book")  # –µ–¥–∏–Ω—ã–π –∫–ª—é—á –ø–æ –≤—Å–µ–º—É –ø–∞–π–ø–ª–∞–π–Ω—É
        if not id_book:
            continue
        text_snippet = (d.page_content or "").strip()[:600]
        page = m.get("page")

        bucket = vector_cards_by_book.setdefault(id_book, {"pages": [], "text_snippets": []})
        bucket["pages"].append(page)
        bucket["text_snippets"].append(text_snippet)

    # --- –û–±–æ–≥–∞—â–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –ø—É—Ç–∞–Ω–∏—Ü–∞ —Å doc_id/id_book) ---
    # –ë–µ—Ä—ë–º –≤—Å–µ id_book, —á—Ç–æ –Ω–∞–∫–æ–ø–∏–ª–∏ –≤—ã—à–µ
    all_id_books = list(vector_cards_by_book.keys())

    def fetch_doc_meta(id_books: list[str]):
        with SessionLocal() as session:
            # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ id_book –∏ source
            docs = session.query(Document).filter(Document.id_book.in_(id_books)).all()

            # –ß—Ç–æ–±—ã –Ω–µ –¥–µ–ª–∞—Ç—å –ø–æ –¥–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤—Å—è–∫–∏–π —Ä–∞–∑
            kabis_ids = [d.id_book for d in docs if d.source == "kabis"]
            lib_ids   = [d.id_book for d in docs if d.source == "library"]

            kabis_map = {}
            if kabis_ids:
                for rec in session.query(Kabis).filter(Kabis.id_book.in_(kabis_ids)).all():
                    kabis_map[rec.id_book] = rec

            lib_map = {}
            if lib_ids:
                for rec in session.query(Library).filter(Library.id_book.in_(lib_ids)).all():
                    lib_map[rec.id_book] = rec

            enriched = {}
            for d in docs:
                if d.source == "kabis" and d.id_book in kabis_map:
                    r = kabis_map[d.id_book]
                    enriched[d.id_book] = {
                        "title": (r.title or r.author),
                        "language": r.lang,
                        "pub_info": r.pub_info,
                        "subjects": r.subjects,
                        "download_url": (str(r.download_url) if r.download_url else None),
                        "source": "kabis",
                        "id_book": d.id_book,
                    }
                elif d.source == "library" and d.id_book in lib_map:
                    r = lib_map[d.id_book]
                    enriched[d.id_book] = {
                        "title": r.title,
                        "download_url": (r.download_url or None),
                        "source": "library",
                        "id_book": d.id_book,
                    }
            return enriched

    enriched_meta = await asyncio.to_thread(fetch_doc_meta, all_id_books)

    # –°–ª–∏–≤–∞–µ–º enrichment –≤ –∫–∞—Ä—Ç–æ—á–∫–∏
    for id_book, meta in enriched_meta.items():
        vector_cards_by_book.setdefault(id_book, {})
        vector_cards_by_book[id_book].update(meta)

    # --- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ä—Ç–æ—á–µ–∫ (—É –≤–∞—Å —É–∂–µ async) ---
    # –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —á–∏—Å—Ç—ã–µ dict
    tasks = [summarize_card(llm, req, {"id_book": bid, **card}) for bid, card in vector_cards_by_book.items()]
    annotated_vector_cards = await asyncio.gather(*tasks)

    # --- –õ–æ–≥ ---
    save_chat_history(
        db=db,
        session_id=session_id,
        question=req.query,
        answer="",
        tools_used=["book_search", "vector_search", "reranker"]
    )

    return {
        "reply": "–í –±–∏–±–ª–∏–æ—Ç–µ–∫–µ –Ω–∞–π–¥–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∫–Ω–∏–≥–∏:",
        "book_search": kb_map,
        "vector_search": annotated_vector_cards
    }


class LLMContextRequest(BaseModel):
    text_snippet: str
    title: str
    query: str


@router.post("/generate_llm_context")
async def generate_llm_context(payload: LLMContextRequest, current_user: User = Depends(get_current_user)):
    system_role = (
        "–¢—ã ‚Äî –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π –ø–æ–º–æ—â–Ω–∏–∫. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ –æ–±—ä—è—Å–Ω–∏—Ç—å, "
        "–ø–æ—á–µ–º—É –¥–∞–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–µ–Ω —Å—Ç—É–¥–µ–Ω—Ç—É –ø–æ –µ–≥–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏. "
        "–£—á–∏—Ç—ã–≤–∞–π –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ —Ç–µ–±–µ –¥–∞–Ω–Ω—ã–µ –æ —Å—Ç—É–¥–µ–Ω—Ç–µ (–§–ò–û –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å) –∏ –≤–æ–ø—Ä–æ—Å, –∫–æ—Ç–æ—Ä—ã–π –æ–Ω –∏–∑—É—á–∞–µ—Ç. "
        "–û—Ç–≤–µ—Ç –¥–∞–≤–∞–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ: —Å—Ç—Ä. X ‚Äî –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, –≥–¥–µ X ‚Äî –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã. "
        "–ü–æ—è—Å–Ω—è–π, –∫–∞–∫—É—é —Ü–µ–Ω–Ω–æ—Å—Ç—å –Ω–µ—Å—ë—Ç –∫–∞–∂–¥–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–º–µ–Ω–Ω–æ –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–∞ —Å –¥–∞–Ω–Ω–æ–π —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å—é."
    )

    human_msg = (
        f"–í–æ–ø—Ä–æ—Å: {payload.query}\n\n"
        f"–ò—Å—Ç–æ—á–Ω–∏–∫: {payload.title}\n\n"
        f"–§—Ä–∞–≥–º–µ–Ω—Ç: {payload.text_snippet}"
        f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {current_user.full_name}, –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {current_user.educational_program}"
    )
    print(current_user.full_name)
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–¥ –≤–∞—à LLM/LC
    from langchain.prompts import ChatPromptTemplate
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_role),
        ("human", human_msg),
    ])
    msgs = prompt.format_messages()

    llm = get_llm()  # –î–û–õ–ñ–ï–ù –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å streaming=True

    async def gen():
        yield " \n"

        async for chunk in llm.astream(msgs):
            text = getattr(chunk, "content", None)
            if text:
                yield text
                await asyncio.sleep(0)

        yield "\n"

    headers = {
        "Cache-Control": "no-cache, no-transform",
        "X-Accel-Buffering": "no",  # –¥–ª—è nginx
        "Connection": "keep-alive",
    }
    return StreamingResponse(gen(), media_type="text/plain; charset=utf-8", headers=headers)