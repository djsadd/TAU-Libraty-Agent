from fastapi import APIRouter, Depends
from pydantic import BaseModel
from fastapi.responses import JSONResponse
import re
from sqlalchemy import text

from app.core.security import get_current_user
from app.models.user import User
import asyncio
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.tools import tool
from sqlalchemy.orm import Session
from app.models.kabis import Kabis
from app.models.libtau import Library
import time
from fastapi import HTTPException
from app.core.config import settings
from sshtunnel import SSHTunnelForwarder
import mysql.connector

from pydantic import BaseModel
from typing import List, Optional
from ...deps import get_retriever_dep, get_llm, get_book_retriever_dep
from app.models.chat import ChatHistory
from app.core.db import SessionLocal
import re
from fastapi.responses import StreamingResponse
from app.models.books import Document
from sentence_transformers import CrossEncoder


router = APIRouter(prefix="/api", tags=["chat", "chat_card", "educational_discipline_list"])
reranker = CrossEncoder("BAAI/bge-reranker-v2-m3", device="cuda")


di_iin = {
    "021205551147": ["–ö—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –∑–∞—â–∏—Ç—ã –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
                     "–ú–µ–Ω–µ–¥–∂–º–µ–Ω—Ç",
                     "–ò—Å—Ç–æ—Ä–∏—è –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞",
                     "–§–∏–ª–æ—Å–æ—Ñ–∏—è",
                     "–°–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö",
                     "–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ —Å–µ—Ç–∏",
                     ]
}


def clean_context(text: str) -> str:
    """
    –£–¥–∞–ª—è–µ—Ç —Ä–∞–∑–¥–µ–ª—ã —Å–æ —Å–ø–∏—Å–∫–∞–º–∏ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –∏ –≤–Ω–µ—à–Ω–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
    (–≤–∫–ª—é—á–∞—è '–°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã', '–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', 'References' –∏ –∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç—ã)
    –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫–Ω–∏–≥–∏.
    """
    # –°–Ω–∞—á–∞–ª–∞ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã (—á—Ç–æ–±—ã '–ª–∏—Ç–µ—Ä –∞—Ç—É—Ä–∞' —Å—Ç–∞–ª–æ '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞')
    normalized = re.sub(r'\s+', ' ', text)

    # –£–¥–∞–ª—è–µ–º –≤—Å—ë, —á—Ç–æ –∏–¥—ë—Ç –ø–æ—Å–ª–µ —Ç–∏–ø–∏—á–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å–ø–∏—Å–∫–æ–≤ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã
    pattern = r'(?i)(—Å–ø–∏—Å–æ–∫\s+—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–æ–π\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã|—Å–ø–∏—Å–æ–∫\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã|–æ—Å–Ω–æ–≤–Ω–∞—è\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞|–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞|–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞|references)\b.*'
    cleaned = re.sub(pattern, '', normalized)

    return cleaned.strip()


def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


class ChatHistoryItem(BaseModel):
    sessionId: str
    question: str
    answer: str
    tools_used: Optional[List[str]] = []
    timestamp: Optional[float] = None  # UNIX timestamp


def save_chat_history(db: Session, session_id: str, question: str, answer: str, tools_used: list[str]):
    item = ChatHistory(
        session_id=session_id,
        question=question,
        answer=answer,
        tools_used=tools_used,
        timestamp=time.time()
    )
    db.add(item)
    db.commit()
    db.refresh(item)
    return item


class ChatRequest(BaseModel):
    query: str
    k: int | None = None
    sessionId: Optional[str] = None  # –Ω–æ–≤–æ–µ –ø–æ–ª–µ


def _format_docs(docs, per_chunk_chars=800, max_chunks=5):
    lines = []
    for d in docs[:max_chunks]:
        m = d.metadata or {}
        title = m.get("title", "–∫–Ω–∏–≥–∞")
        # author = m.get("author", "–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω")
        # subject = m.get("subject")
        page = m.get("page", "?")
        text = (d.page_content or "")[:per_chunk_chars].strip()
        lines.append(f"[{title}, —Å—Ç—Ä. {page}] {text}")
        print(title, m.get("source"), text)
    return "\n\n".join(lines)


# ---- –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç: –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ ----
@tool("vector_search", return_direct=False)
def vector_search(query: str, k: int = 5, retriever=None) -> str:
    """
    –ü–æ–∏—Å–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ –ø–æ —Å–º—ã—Å–ª–æ–≤–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫—É—Å–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∫–Ω–∏–≥–∏, –∞–≤—Ç–æ—Ä–∞ –∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    """
    print("–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫")
    if retriever is None:
        return "Retriever –Ω–µ –ø–æ–¥–∫–ª—é—á—ë–Ω"
    docs = retriever.invoke(query, config={"k": k})
    return _format_docs(docs)


def _format_books(docs, max_items=None):
    lines = []
    limit = max_items or len(docs)   # –µ—Å–ª–∏ None ‚Üí –±–µ—Ä–µ–º –≤—Å–µ
    for d in docs[:limit]:
        m = d.metadata or {}
        title = m.get("title", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–Ω–∏–≥–∞")
        # subject = m.get("subject", "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
        print(title, m.get("title"), m.get("id_book"))

        lines.append(f"üìò {title}\n")
    return "\n\n".join(lines)


@tool("book_search")
def book_search(query: str, k: int = 50, retriever=None) -> str:
    """
    –û–±–∑–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–Ω–∏–≥–∞–º (—ç–º–±–µ–¥–¥–∏–Ω–≥–∏).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–Ω–∏–≥ (–º–Ω–æ–≥–æ).
    """
    print("–û–±–∑–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫")
    if retriever is None:
        return "Retriever –¥–ª—è –∫–Ω–∏–≥ –Ω–µ –ø–æ–¥–∫–ª—é—á—ë–Ω"

    docs = retriever.invoke(query, config={"k": k})
    return _format_books(docs, max_items=k)


# –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
# –∫–ª—é—á: sessionId, –∑–Ω–∞—á–µ–Ω–∏–µ: timestamp –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
_last_request_time: dict[str, float] = {}

RATE_LIMIT_SECONDS = 5  # –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏


@router.post("/chat", summary="–ß–∞—Ç —Å –ò–ò")
async def chat(req: ChatRequest,
               retriever=Depends(get_retriever_dep),
               book_retriever=Depends(get_book_retriever_dep),
               llm=Depends(get_llm),
               db: Session = Depends(get_db)):

    session_id = req.sessionId or "anonymous"

    vs_tool_used = []
    bs_tool_used = []

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–∞
    now = time.time()
    last_time = _last_request_time.get(session_id, 0)

    if now - last_time < RATE_LIMIT_SECONDS:
        return JSONResponse(
            {"error": f"–°–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ {int(RATE_LIMIT_SECONDS - (now - last_time))} —Å–µ–∫."},
            status_code=429
        )

    _last_request_time[session_id] = now

    # tools
    vs_tool = lambda q, k=5: (vs_tool_used.append("vector_search") or vector_search.func(q, k, retriever=retriever))
    bs_tool = lambda q, k=100: (bs_tool_used.append("book_search") or book_search.func(q, k, retriever=book_retriever))

    # –û–±—â–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    system_prompt = (
        "–¢—ã ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –ø–æ–∏—Å–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–Ω–∏–≥–∞—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ ¬´–¢—É—Ä–∞–Ω-–ê—Å—Ç–∞–Ω–∞¬ª.\n"
        "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ ¬´–ö–æ–Ω—Ç–µ–∫—Å—Ç¬ª, –≥–¥–µ —É–∫–∞–∑–∞–Ω—ã –∫–Ω–∏–≥–∏ –∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.\n"
        "–ö–∞–∂–¥—ã–π —Ñ–∞–∫—Ç –∏–ª–∏ –≤—ã–≤–æ–¥ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–π —Å—Å—ã–ª–∫–æ–π –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n"
        "¬´(<i>–Ω–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏</i>, —Å—Ç—Ä. N)¬ª.\n"
        "–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç ‚Äî —á–µ—Å—Ç–Ω–æ —Å–æ–æ–±—â–∞–π: "
        "¬´–í –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ –¢—É—Ä–∞–Ω-–ê—Å—Ç–∞–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç.¬ª\n"
        "–§–æ—Ä–º–∞—Ç–∏—Ä—É–π –æ—Ç–≤–µ—Ç –≤ HTML —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ–≥–æ–≤ (<p>, <ul>, <li>, <b>, <i>).\n"
        "–ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –∫–Ω–∏–≥–∏ –∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ —É–∫–∞–∑–∞–Ω—ã –≤ —Ä–∞–∑–¥–µ–ª–µ ¬´–ö–æ–Ω—Ç–µ–∫—Å—Ç¬ª."
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        (
            "human",
            "–í–æ–ø—Ä–æ—Å —Å—Ç—É–¥–µ–Ω—Ç–∞: {question}\n\n"
            "–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∫–Ω–∏–≥–∏ –∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã):\n{context}"
        ),
    ])

    # --- 1Ô∏è‚É£ –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å ‚Äî —á–µ—Ä–µ–∑ vector_search ---
    vector_chain = (
        RunnableParallel(
            question=RunnablePassthrough(),
            context=lambda q: clean_context(vs_tool(q, req.k or 5)),
        )
        | prompt
        | llm
    )
    vector_answer = vector_chain.invoke(req.query).content

    # --- 2Ô∏è‚É£ –í—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å ‚Äî —á–µ—Ä–µ–∑ book_search ---
    book_chain = (
        RunnableParallel(
            question=RunnablePassthrough(),
            context=lambda q: bs_tool(q, req.k or 100),
        )
        | prompt
        | llm
    )
    book_answer = book_chain.invoke(req.query).content

    # --- 3Ô∏è‚É£ –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Ç–≤–µ—Ç—ã ---
    final_answer = (
        "<h3>–û—Ç–≤–µ—Ç –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫):</h3>\n"
        f"{vector_answer}\n"
        "<hr>"
        "<h3>–û—Ç–≤–µ—Ç –ø–æ –∫–Ω–∏–≥–∞–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:</h3>\n"
        f"{book_answer}"
    )

    # --- 4Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î ---
    save_chat_history(
        db=db,
        session_id=session_id,
        question=req.query,
        answer=final_answer,
        tools_used=list(set(vs_tool_used + bs_tool_used)),
    )

    return {"reply": final_answer}


async def summarize_card(llm, card):
    key, value = card
    context = ''
    cards = {

    }
    for i in range(len(value["pages"])):
        context += "—Å—Ç—Ä." + str(value["pages"][i]) + "\n"
        context += "—Ñ—Ä–∞–≥–º–µ–Ω—Ç" + value["text_snippets"][i] + "\n"
    return {
        'title': value["title"],
        'download_url': value['download_url'],
        "text_snippet": context,
            }


@router.post("/chat_card", summary="–ß–∞—Ç —Å –∫–∞—Ä—Ç–æ—á–∫–∞–º–∏ –∫–Ω–∏–≥")
async def chat(req: ChatRequest,
               retriever=Depends(get_retriever_dep),
               book_retriever=Depends(get_book_retriever_dep),
               llm=Depends(get_llm),
               db: Session = Depends(get_db),
               current_user: User = Depends(get_current_user)):

    session_id = req.sessionId or "anonymous"

    # --- –∑–∞—â–∏—Ç–∞ –æ—Ç —Å–ø–∞–º–∞ ---
    now = time.time()
    last_time = _last_request_time.get(session_id, 0)
    if now - last_time < RATE_LIMIT_SECONDS:
        return JSONResponse(
            {"error": f"–°–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ {int(RATE_LIMIT_SECONDS - (now - last_time))} —Å–µ–∫."},
            status_code=429
        )
    _last_request_time[session_id] = now

    # --- BOOK SEARCH ---
    book_docs = book_retriever.invoke(req.query, config={"k": 1000})
    id_books = [d.metadata.get("id_book") for d in book_docs if d.metadata]
    with SessionLocal() as session:
        kabis_records = session.query(Kabis).filter(Kabis.id_book.in_(id_books)).all()
        kb_map = [
            {
                "Language": k.lang,
                "title": f"{k.author} {k.title}",
                "pub_info": k.pub_info,
                "year": k.year,
                "subjects": k.subjects,
                "source": "book_search"
            }
            for k in kabis_records
        ]

    # --- –í–ï–ö–¢–û–†–ù–´–ô –ü–û–ò–°–ö ---
    vec_docs = retriever.invoke(req.query, config={"k": 50})  # —á—É—Ç—å –±–æ–ª—å—à–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤

    # --- –†–ï–†–ê–ù–ö–ï–†: —Å–æ—Ä—Ç–∏—Ä—É–µ–º vec_docs –ø–æ —Å–º—ã—Å–ª–æ–≤–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ ---
    pairs = [(req.query, d.page_content or "") for d in vec_docs]
    scores = reranker.predict(pairs)

    for d, s in zip(vec_docs, scores):
        d.metadata["rerank_score"] = float(s)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é
    vec_docs = sorted(vec_docs, key=lambda x: x.metadata.get("rerank_score", 0), reverse=True)

    # --- –°–æ–±–∏—Ä–∞–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ ---
    vector_cards_dictionary = {}
    for d in vec_docs:
        m = d.metadata or {}
        id_book = m.get("id_book")
        text_snippet = (d.page_content or "")[:600].strip()
        page = m.get("page")

        if id_book not in vector_cards_dictionary:
            vector_cards_dictionary[id_book] = {
                "pages": [],
                "text_snippets": [],
            }

        vector_cards_dictionary[id_book]['pages'].append(page)
        vector_cards_dictionary[id_book]['text_snippets'].append(text_snippet)

    # --- –û–±–æ–≥–∞—â–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –ë–î ---
    id_books = [d.metadata.get("doc_id") for d in vec_docs if d.metadata]
    with SessionLocal() as session:
        documents = session.query(Document).filter(Document.id.in_(id_books)).all()
        for doc in documents:
            if doc.source == 'kabis':
                record = session.query(Kabis).filter_by(id=doc.id_book).first()
                vector_cards_dictionary[doc.id_book].update({
                    "title": record.title or record.author,
                    "language": record.lang,
                    "pub_info": record.pub_info,
                    "subjects": record.subjects,
                    "download_url": "kabis.tau-edu.kz" + str(record.download_url)
                })
            elif doc.source == 'library':
                record = session.query(Library).filter_by(id=doc.id_book).first()
                vector_cards_dictionary[doc.id_book].update({
                    "title": record.title,
                    "download_url": record.download_url
                })

    # --- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ---
    tasks = [summarize_card(llm, card) for card in vector_cards_dictionary.items()]
    annotated_vector_cards = await asyncio.gather(*tasks)

    # --- –õ–æ–≥–∏—Ä—É–µ–º ---
    save_chat_history(
        db=db,
        session_id=session_id,
        question=req.query,
        answer="",
        tools_used=["book_search", "vector_search", "reranker"]
    )

    return {
        "reply": "–í –±–∏–±–ª–∏–æ—Ç–µ–∫–µ –Ω–∞–π–¥–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∫–Ω–∏–≥–∏: ",
        "book_search": kb_map,
        "vector_search": annotated_vector_cards
    }


@router.get("/educational_discipline_list")
async def educational_program_list(
    current_user: User = Depends(get_current_user)
):
    return get_disciplines_from_platonus(current_user)


async def process_row(row, retriever, book_retriever, llm):
    # --- book search ---
    book_docs = book_retriever.invoke(row, config={"k": 5})
    id_books = [d.metadata.get("id_book") for d in book_docs if d.metadata]

    # --- —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –±–ª–æ–∫ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö ---
    def fetch_kabis():
        with SessionLocal() as session:
            kabis_records = session.query(Kabis).filter(Kabis.id_book.in_(id_books)).all()
            return [
                {
                    "Language": k.lang,
                    "title": f"{k.author} {k.title}",
                    "pub_info": k.pub_info,
                    "year": k.year,
                    "subjects": k.subjects,
                    "source": "book_search"
                }
                for k in kabis_records
            ]

    kb_map = await asyncio.to_thread(fetch_kabis)

    # --- vector retrieval (–±–µ–∑ reranker) ---
    vec_docs = retriever.invoke(row, config={"k": 5})

    vector_cards_dictionary = {}
    for d in vec_docs:
        m = d.metadata or {}
        id_book = m.get("id_book")
        text_snippet = (d.page_content or "")[:600].strip()
        page = m.get("page")
        if id_book not in vector_cards_dictionary:
            vector_cards_dictionary[id_book] = {"pages": [], "text_snippets": []}
        vector_cards_dictionary[id_book]["pages"].append(page)
        vector_cards_dictionary[id_book]["text_snippets"].append(text_snippet)

    # --- enrich metadata from DB ---
    def enrich_from_db():
        id_books_local = [d.metadata.get("doc_id") for d in vec_docs if d.metadata]
        with SessionLocal() as session:
            documents = session.query(Document).filter(Document.id.in_(id_books_local)).all()
            for doc in documents:
                if doc.source == 'kabis':
                    record = session.query(Kabis).filter_by(id=doc.id_book).first()
                    if record:
                        vector_cards_dictionary[doc.id_book].update({
                            "title": record.title or record.author,
                            "language": record.lang,
                            "pub_info": record.pub_info,
                            "subjects": record.subjects,
                            "download_url": "kabis.tau-edu.kz" + str(record.download_url)
                        })
                elif doc.source == 'library':
                    record = session.query(Library).filter_by(id=doc.id_book).first()
                    if record:
                        vector_cards_dictionary[doc.id_book].update({
                            "title": record.title,
                            "download_url": record.download_url
                        })

    await asyncio.to_thread(enrich_from_db)

    # --- summarize ---
    tasks = [summarize_card(llm, card) for card in vector_cards_dictionary.items()]
    annotated_vector_cards = await asyncio.gather(*tasks)

    return {
        "reply": "–í –±–∏–±–ª–∏–æ—Ç–µ–∫–µ –Ω–∞–π–¥–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∫–Ω–∏–≥–∏: ",
        "book_search": kb_map,
        "vector_search": annotated_vector_cards
    }


@router.get("/chat_card_recommendations", summary="–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∫–Ω–∏–≥")
async def chat_card_recommendations(
               retriever=Depends(get_retriever_dep),
               book_retriever=Depends(get_book_retriever_dep),
               llm=Depends(get_llm),
               db: Session = Depends(get_db),
               current_user: User = Depends(get_current_user)):

    user_iin = current_user.iin
    rows = di_iin[user_iin]

    tasks = [process_row(row, retriever, book_retriever, llm) for row in rows]
    results = await asyncio.gather(*tasks)

    cards = {row: result for row, result in zip(rows, results)}
    return cards


class LLMContextRequest(BaseModel):
    text_snippet: str
    title: str
    query: str
#


@router.post("/generate_llm_context")
async def generate_llm_context(payload: LLMContextRequest, current_user: User = Depends(get_current_user)):
    system_role = (
        "–¢—ã ‚Äî –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π –ø–æ–º–æ—â–Ω–∏–∫. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ –æ–±—ä—è—Å–Ω–∏—Ç—å, "
        "–ø–æ—á–µ–º—É –¥–∞–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–µ–Ω —Å—Ç—É–¥–µ–Ω—Ç—É –ø–æ –µ–≥–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏. "
        "–£—á–∏—Ç—ã–≤–∞–π –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ —Ç–µ–±–µ –¥–∞–Ω–Ω—ã–µ –æ —Å—Ç—É–¥–µ–Ω—Ç–µ (–§–ò–û –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å) –∏ –≤–æ–ø—Ä–æ—Å, –∫–æ—Ç–æ—Ä—ã–π –æ–Ω –∏–∑—É—á–∞–µ—Ç. "
        "–û—Ç–≤–µ—Ç –¥–∞–≤–∞–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ: —Å—Ç—Ä. X ‚Äî –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, –≥–¥–µ X ‚Äî –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã. "
        "–ü–æ—è—Å–Ω—è–π, –∫–∞–∫—É—é —Ü–µ–Ω–Ω–æ—Å—Ç—å –Ω–µ—Å—ë—Ç –∫–∞–∂–¥–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–º–µ–Ω–Ω–æ –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–∞ —Å –¥–∞–Ω–Ω–æ–π —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å—é."
    )

    human_msg = (
        f"–í–æ–ø—Ä–æ—Å: {payload.query}\n\n"
        f"–ò—Å—Ç–æ—á–Ω–∏–∫: {payload.title}\n\n"
        f"–§—Ä–∞–≥–º–µ–Ω—Ç: {payload.text_snippet}"
        f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {current_user.full_name}, –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {current_user.educational_program}"
    )
    print(current_user.full_name)
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–¥ –≤–∞—à LLM/LC
    from langchain.prompts import ChatPromptTemplate
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_role),
        ("human", human_msg),
    ])
    msgs = prompt.format_messages()

    llm = get_llm()  # –î–û–õ–ñ–ï–ù –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å streaming=True

    async def gen():
        yield " \n"

        async for chunk in llm.astream(msgs):
            text = getattr(chunk, "content", None)
            if text:
                yield text
                await asyncio.sleep(0)

        yield "\n"

    headers = {
        "Cache-Control": "no-cache, no-transform",
        "X-Accel-Buffering": "no",  # –¥–ª—è nginx
        "Connection": "keep-alive",
    }
    return StreamingResponse(gen(), media_type="text/plain; charset=utf-8", headers=headers)


@router.get("/students/disciplines")
def get_disciplines_from_platonus(
        current_user: User = Depends(get_current_user)
):
    conn = mysql.connector.connect(
        host=settings.PLATONUS_DB_HOST,
        port=int(settings.PLATONUS_DB_PORT),
        user=settings.PLATONUS_DB_USER,
        password=settings.PLATONUS_DB_PASSWORD,
        database=settings.PLATONUS_DB_NAME
    )

    iin = current_user.iin

    cursor = conn.cursor(dictionary=True)

    cursor.execute("SELECT DATABASE(), VERSION();")
    print("–¢–µ–∫—É—â–∞—è –±–∞–∑–∞ –∏ –≤–µ—Ä—Å–∏—è:", cursor.fetchone())

    query = """
        SELECT 
            subjects.SubjectNameRU AS discipline
        FROM journal j
        JOIN students ON j.StudentID = students.StudentID
        JOIN studygroups ON j.StudyGroupID = studygroups.StudyGroupID
        JOIN subjects ON subjects.SubjectID = studygroups.subjectid
        WHERE j.markTypeID IN (2, 3, 4)
          AND year = 2025
          AND students.iinplt = %s
        GROUP BY discipline;
    """

    cursor.execute(query, (iin,))
    results = cursor.fetchall()

    cursor.close()
    conn.close()
    disciplines = [row["discipline"] for row in results]

    return {"disciplines": disciplines}
